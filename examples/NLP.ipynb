{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring NLP Models with Skater/LIME\n",
    "\n",
    "In this example, we'll train a couple types of models, and use Skater, LIME, and ipywidgets to interactively explore model behavior.\n",
    "\n",
    "### Install Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting np_utils\n",
      "  Downloading np_utils-0.5.3.4.tar.gz (56kB)\n",
      "\u001b[K    100% |################################| 61kB 2.6MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: numpy>=1.0 in /usr/local/lib/python2.7/dist-packages (from np_utils)\n",
      "Collecting future>=0.16 (from np_utils)\n",
      "  Downloading future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |################################| 829kB 1.4MB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: np-utils, future\n",
      "  Running setup.py bdist_wheel for np-utils ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/5d/be/b5/e223535ec3efb733df6afc2518d90d398bbe759e665683b025\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a\n",
      "Successfully built np-utils future\n",
      "Installing collected packages: future, np-utils\n",
      "Successfully installed future-0.16.0 np-utils-0.5.3.4\n",
      "Collecting theano\n",
      "  Downloading Theano-0.9.0.tar.gz (3.1MB)\n",
      "\u001b[K    100% |################################| 3.1MB 430kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from theano)\n",
      "Requirement already up-to-date: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from theano)\n",
      "Collecting six>=1.9.0 (from theano)\n",
      "  Downloading six-1.11.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: theano\n",
      "  Running setup.py bdist_wheel for theano ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/d5/5b/93/433299b86e3e9b25f0f600e4e4ebf18e38eb7534ea518eba13\n",
      "Successfully built theano\n",
      "Installing collected packages: six, theano\n",
      "  Found existing installation: six 1.8.0\n",
      "    Uninstalling six-1.8.0:\n",
      "      Successfully uninstalled six-1.8.0\n",
      "Successfully installed six-1.11.0 theano-0.9.0\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-1.3.0-cp27-cp27mu-manylinux1_x86_64.whl (43.1MB)\n",
      "\u001b[K    100% |################################| 43.1MB 30kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting protobuf>=3.3.0 (from tensorflow)\n",
      "  Downloading protobuf-3.4.0-cp27-cp27mu-manylinux1_x86_64.whl (6.2MB)\n",
      "\u001b[K    100% |################################| 6.2MB 204kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Collecting wheel (from tensorflow)\n",
      "  Downloading wheel-0.30.0-py2.py3-none-any.whl (49kB)\n",
      "\u001b[K    100% |################################| 51kB 10.3MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting backports.weakref>=1.0rc1 (from tensorflow)\n",
      "  Downloading backports.weakref-1.0.post1-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: numpy>=1.11.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow)\n",
      "Collecting mock>=2.0.0 (from tensorflow)\n",
      "  Downloading mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |################################| 61kB 10.5MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-tensorboard<0.2.0,>=0.1.0 (from tensorflow)\n",
      "  Downloading tensorflow_tensorboard-0.1.6-py2-none-any.whl (2.2MB)\n",
      "\u001b[K    100% |################################| 2.2MB 610kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting setuptools (from protobuf>=3.3.0->tensorflow)\n",
      "  Downloading setuptools-36.5.0-py2.py3-none-any.whl (478kB)\n",
      "\u001b[K    100% |################################| 481kB 2.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow)\n",
      "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow)\n",
      "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99kB)\n",
      "\u001b[K    100% |################################| 102kB 11.2MB/s a 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Collecting markdown>=2.6.8 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "  Downloading Markdown-2.6.9.tar.gz (271kB)\n",
      "\u001b[K    100% |################################| 276kB 5.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting html5lib==0.9999999 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "  Downloading html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |################################| 890kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting bleach==1.5.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl\n",
      "Building wheels for collected packages: markdown, html5lib\n",
      "  Running setup.py bdist_wheel for markdown ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/bf/46/10/c93e17ae86ae3b3a919c7b39dad3b5ccf09aeb066419e5c1e5\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/6f/85/6c/56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\n",
      "Successfully built markdown html5lib\n",
      "Installing collected packages: setuptools, protobuf, wheel, backports.weakref, pbr, mock, markdown, html5lib, bleach, tensorflow-tensorboard, tensorflow\n",
      "  Found existing installation: setuptools 36.4.0\n",
      "    Uninstalling setuptools-36.4.0:\n",
      "      Successfully uninstalled setuptools-36.4.0\n",
      "  Found existing installation: wheel 0.24.0\n",
      "    Uninstalling wheel-0.24.0:\n",
      "      Successfully uninstalled wheel-0.24.0\n",
      "  Found existing installation: html5lib 0.999999999\n",
      "    Uninstalling html5lib-0.999999999:\n",
      "      Successfully uninstalled html5lib-0.999999999\n",
      "  Found existing installation: bleach 2.0.0\n",
      "    Uninstalling bleach-2.0.0:\n",
      "      Successfully uninstalled bleach-2.0.0\n",
      "Successfully installed backports.weakref-1.0.post1 bleach-1.5.0 html5lib-0.9999999 markdown-2.6.9 mock-2.0.0 pbr-3.1.1 protobuf-3.4.0 setuptools-36.5.0 tensorflow-1.3.0 tensorflow-tensorboard-0.1.6 wheel-0.30.0\n",
      "Collecting keras==2.0.6\n",
      "  Downloading Keras-2.0.6.tar.gz (228kB)\n",
      "\u001b[K    100% |################################| 235kB 3.2MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: theano in /usr/local/lib/python2.7/dist-packages (from keras==2.0.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python2.7/dist-packages (from keras==2.0.6)\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from keras==2.0.6)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python2.7/dist-packages (from theano->keras==2.0.6)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python2.7/dist-packages (from theano->keras==2.0.6)\n",
      "Building wheels for collected packages: keras\n",
      "  Running setup.py bdist_wheel for keras ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/c2/80/ba/2beab8c2131e2dcc391ee8a2f55e648af66348115c245e0839\n",
      "Successfully built keras\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.0.6\n",
      "Collecting spacy\n",
      "  Downloading spacy-1.9.0.tar.gz (3.4MB)\n",
      "\u001b[K    100% |################################| 3.4MB 391kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Collecting murmurhash<0.27,>=0.26 (from spacy)\n",
      "  Downloading murmurhash-0.26.4-cp27-cp27mu-manylinux1_x86_64.whl\n",
      "Collecting cymem<1.32,>=1.30 (from spacy)\n",
      "  Downloading cymem-1.31.2-cp27-cp27mu-manylinux1_x86_64.whl (66kB)\n",
      "\u001b[K    100% |################################| 71kB 10.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting preshed<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading preshed-1.0.0.tar.gz (89kB)\n",
      "\u001b[K    100% |################################| 92kB 10.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting thinc<6.6.0,>=6.5.0 (from spacy)\n",
      "  Downloading thinc-6.5.2.tar.gz (926kB)\n",
      "\u001b[K    100% |################################| 931kB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy)\n",
      "  Downloading plac-0.9.6-py2.py3-none-any.whl\n",
      "Requirement already satisfied: pip<10.0.0,>=9.0.0 in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Collecting pathlib (from spacy)\n",
      "  Downloading pathlib-1.0.1.tar.gz (49kB)\n",
      "\u001b[K    100% |################################| 51kB 10.1MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting ujson>=1.35 (from spacy)\n",
      "  Downloading ujson-1.35.tar.gz (192kB)\n",
      "\u001b[K    100% |################################| 194kB 5.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python2.7/dist-packages (from spacy)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Downloading requests-2.18.4-py2.py3-none-any.whl (88kB)\n",
      "\u001b[K    100% |################################| 92kB 11.0MB/s ta 0:00:01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25hCollecting regex<2017.12.1,>=2017.4.1 (from spacy)\n",
      "  Downloading regex-2017.07.28.tar.gz (607kB)\n",
      "\u001b[K    100% |################################| 614kB 2.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ftfy<5.0.0,>=4.4.2 (from spacy)\n",
      "  Downloading ftfy-4.4.3.tar.gz (50kB)\n",
      "\u001b[K    100% |################################| 51kB 10.2MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting wrapt (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading wrapt-1.10.11.tar.gz\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading tqdm-4.17.0-py2.py3-none-any.whl (47kB)\n",
      "\u001b[K    100% |################################| 51kB 10.4MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting cytoolz<0.9,>=0.8 (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading cytoolz-0.8.2.tar.gz (386kB)\n",
      "\u001b[K    100% |################################| 389kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting termcolor (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "  Downloading termcolor-1.1.0.tar.gz\n",
      "Collecting idna<2.7,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading idna-2.6-py2.py3-none-any.whl (56kB)\n",
      "\u001b[K    100% |################################| 61kB 10.8MB/s ta 0:00:01\n",
      "\u001b[?25hCollecting urllib3<1.23,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading urllib3-1.22-py2.py3-none-any.whl (132kB)\n",
      "\u001b[K    100% |################################| 133kB 8.8MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "\u001b[K    100% |################################| 143kB 8.5MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: html5lib in /usr/local/lib/python2.7/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python2.7/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python2.7/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy)\n",
      "Building wheels for collected packages: spacy, preshed, thinc, pathlib, ujson, regex, ftfy, wrapt, cytoolz, termcolor\n",
      "  Running setup.py bdist_wheel for spacy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/8c/a9/96/dba5bf6fdd55d8c04e9ffb1b6244137c36e8b33e03d3e66a9a\n",
      "  Running setup.py bdist_wheel for preshed ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/97/64/22/20fabf1f51039b799e64e46d0381b023cfdbe159c349d7c135\n",
      "  Running setup.py bdist_wheel for thinc ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ea/1a/6d/0670caa3a983bebfaccf081d9bb092fbd7871d1c4eff8b2c70\n",
      "  Running setup.py bdist_wheel for pathlib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2a/23/a5/d8803db5d631e9f391fe6defe982a238bf5483062eeb34e841\n",
      "  Running setup.py bdist_wheel for ujson ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/9e/9b/d0/df92653bb5b2664c15d8ee5b99e3f2eb08a034444db8922b2f\n",
      "  Running setup.py bdist_wheel for regex ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/59/6f/80/c0f0aebad616d785f0178de88f9ae957991672856038d75a6c\n",
      "  Running setup.py bdist_wheel for ftfy ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ae/d7/4c/339066248431397227741c7fdc80ad85826188ee9b0c24b4c7\n",
      "  Running setup.py bdist_wheel for wrapt ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/56/e1/0f/f7ccf1ed8ceaabccc2a93ce0481f73e589814cbbc439291345\n",
      "  Running setup.py bdist_wheel for cytoolz ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/52/92/ed/661ecb7a67b42b21fc3dea140abb9ae9b8e94e72f0b3aff6c1\n",
      "  Running setup.py bdist_wheel for termcolor ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/de/f7/bf/1bcac7bf30549e6a4957382e2ecab04c88e513117207067b03\n",
      "Successfully built spacy preshed thinc pathlib ujson regex ftfy wrapt cytoolz termcolor\n",
      "Installing collected packages: murmurhash, cymem, preshed, wrapt, tqdm, cytoolz, plac, termcolor, pathlib, thinc, ujson, idna, urllib3, chardet, requests, regex, ftfy, spacy\n",
      "  Found existing installation: urllib3 1.9.1\n",
      "    Uninstalling urllib3-1.9.1:\n",
      "      Successfully uninstalled urllib3-1.9.1\n",
      "  Found existing installation: chardet 2.3.0\n",
      "    Uninstalling chardet-2.3.0:\n",
      "      Successfully uninstalled chardet-2.3.0\n",
      "  Found existing installation: requests 2.4.3\n",
      "    Uninstalling requests-2.4.3:\n",
      "      Successfully uninstalled requests-2.4.3\n",
      "Successfully installed chardet-3.0.4 cymem-1.31.2 cytoolz-0.8.2 ftfy-4.4.3 idna-2.6 murmurhash-0.26.4 pathlib-1.0.1 plac-0.9.6 preshed-1.0.0 regex-2017.7.28 requests-2.18.4 spacy-1.9.0 termcolor-1.1.0 thinc-6.5.2 tqdm-4.17.0 ujson-1.35 urllib3-1.22 wrapt-1.10.11\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install --upgrade np_utils\n",
    "!sudo pip install --upgrade theano\n",
    "!sudo pip install --upgrade tensorflow\n",
    "!sudo pip install keras==2.0.6\n",
    "!sudo pip install spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Downloading en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
      "\n",
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-1.2.0/en_core_web_sm-1.2.0.tar.gz (52.2MB)\n",
      "\u001b[K    100% |################################| 52.2MB 110.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.0.0,>=1.7.0 in /usr/local/lib/python2.7/dist-packages (from en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: regex<2017.12.1,>=2017.4.1 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: pip<10.0.0,>=9.0.0 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: pathlib in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: thinc<6.6.0,>=6.5.0 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: murmurhash<0.27,>=0.26 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /usr/local/lib/python2.7/dist-packages (from spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /usr/local/lib/python2.7/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python2.7/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python2.7/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python2.7/dist-packages (from thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python2.7/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: html5lib in /usr/local/lib/python2.7/dist-packages (from ftfy<5.0.0,>=4.4.2->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python2.7/dist-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy<2.0.0,>=1.7.0->en-core-web-sm==1.2.0)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-1.2.0\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "\n",
      "    /usr/local/lib/python2.7/dist-packages/en_core_web_sm/en_core_web_sm-1.2.0\n",
      "    --> /usr/local/lib/python2.7/dist-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en').\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Restart kernel\n",
    "from __future__ import absolute_import\n",
    "\n",
    "!sudo python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SpaCy Language Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#gimme data\n",
    "dataset = fetch_20newsgroups()\n",
    "docs = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(docs, y, test_size = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Pretrained Word Embeddings\n",
    "\n",
    "We will use SpaCy's pretrained word embeddings as document representations, and feed these representations into a gradient boosting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.57      0.44      0.49       160\n",
      "           comp.graphics       0.45      0.48      0.46       167\n",
      " comp.os.ms-windows.misc       0.66      0.51      0.58       204\n",
      "comp.sys.ibm.pc.hardware       0.58      0.54      0.56       180\n",
      "   comp.sys.mac.hardware       0.59      0.48      0.53       170\n",
      "          comp.windows.x       0.58      0.72      0.65       182\n",
      "            misc.forsale       0.66      0.75      0.70       175\n",
      "               rec.autos       0.78      0.60      0.68       175\n",
      "         rec.motorcycles       0.58      0.74      0.65       180\n",
      "      rec.sport.baseball       0.72      0.67      0.69       183\n",
      "        rec.sport.hockey       0.73      0.83      0.78       164\n",
      "               sci.crypt       0.68      0.78      0.72       186\n",
      "         sci.electronics       0.65      0.56      0.60       162\n",
      "                 sci.med       0.77      0.91      0.83       184\n",
      "               sci.space       0.72      0.76      0.74       176\n",
      "  soc.religion.christian       0.54      0.83      0.66       171\n",
      "      talk.politics.guns       0.67      0.66      0.67       174\n",
      "   talk.politics.mideast       0.56      0.68      0.62       163\n",
      "      talk.politics.misc       0.59      0.42      0.49       132\n",
      "      talk.religion.misc       0.17      0.01      0.02       107\n",
      "\n",
      "             avg / total       0.62      0.63      0.62      3395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#gimme vectors\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from spacy.tokens.doc import Doc\n",
    "from sklearn.metrics import classification_report\n",
    "import six\n",
    "def doc2vec(x):\n",
    "    if isinstance(x, (six.binary_type, six.string_types)):\n",
    "        return nlp(x, parse = False, entity = False, tag = False).vector\n",
    "    \n",
    "    elif type(x) in [list, tuple, np.ndarray]:\n",
    "        return np.array([doc2vec(six.text_type(doc)) for doc in x])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized Input\") \n",
    "\n",
    "# build a pipeline of text -> vector (transformer), vector -> predictions (model)\n",
    "model = GradientBoostingClassifier(n_estimators = 50)\n",
    "model = LogisticRegression()\n",
    "\n",
    "transformer = FunctionTransformer(func = doc2vec, validate=False)\n",
    "pipeline = make_pipeline(transformer, model)\n",
    "pipeline.fit(docs_train, y_train)       \n",
    "\n",
    "#Classification Report on Holdout\n",
    "print(\n",
    "    classification_report(y_test, \n",
    "                          pipeline.predict(docs_test), \n",
    "                          target_names=dataset.target_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: CNN \n",
    "In this model, we convert text to a list of padded lists of word IDs, to be used in an embedding lookup table. The embeddings will be trained as part of a CNN implemented with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, corpus, nlp=None, max_len=200, max_vocab_size=20000):\n",
    "        \"\"\"\n",
    "        corpus: list of strings\n",
    "            Documents used to initialize vocabulary.\n",
    "            \n",
    "        nlp: Spacy language model\n",
    "            If none then will build one in __init__\n",
    "            \n",
    "        max_len: int\n",
    "            Maximum length of a document sequence. Balance information with scale of data.\n",
    "            \n",
    "        max_vocab_size: int\n",
    "        \n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        self.PADDING_VAL = 1\n",
    "        self.MISSING_VAL = 2\n",
    "        self.START_VAL = 3\n",
    "        self.END_VAL = 4\n",
    "        self.vocab = {}\n",
    "        self.vocab_counts = Counter()#Counter(['PADDING_VAL','MISSING_VAL','START_VAL','END_VAL'])\n",
    "        self.build_vocab(corpus)\n",
    "\n",
    "        \n",
    "    def pad(self, obj):\n",
    "        n_pads = max(self.max_len - len(obj) - 2, 0)\n",
    "        we_can_take = self.max_len - 2\n",
    "        result = [self.START_VAL] + obj[:we_can_take] + [self.END_VAL] + [self.PADDING_VAL] * n_pads\n",
    "        return result\n",
    "        \n",
    "    def get_current_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def update(self, words):\n",
    "        for word in words:\n",
    "            self.vocab_counts.update([word])\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        self.vocab = {}\n",
    "        self.vocab_counts = Counter()\n",
    "        \n",
    "        for doc in nlp.tokenizer.pipe(map(six.text_type, corpus)):\n",
    "            self.update(map(self._process_token, doc))\n",
    "            \n",
    "        for i, (word, count) in enumerate(self.vocab_counts.most_common(self.max_vocab_size)):\n",
    "            self.vocab[word] = i\n",
    "        \n",
    "    def _process_token(self, token):\n",
    "        if token.is_space:\n",
    "            return \"SPACE\"\n",
    "        elif token.is_punct:\n",
    "            return \"PUNCT\"       \n",
    "        elif token.like_url:\n",
    "            return \"URL\"\n",
    "        elif token.like_email:\n",
    "            return \"EMAIL\"\n",
    "        elif token.like_num:\n",
    "            return \"NUM\"\n",
    "        else:\n",
    "            return token.lower_\n",
    "\n",
    "    def process_token(self, token):\n",
    "        return self.vocab.get(self._process_token(token), self.MISSING_VAL)\n",
    "\n",
    "    def process(self, texts):\n",
    "        docs = []\n",
    "        for doc in self.nlp.tokenizer.pipe(list(texts)):\n",
    "            docs.append(self.pad(list(map(self.process_token, doc))))\n",
    "        return np.array(docs)\n",
    "            \n",
    "    def __call__(self, texts):\n",
    "        return self.process(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "7919/7919 [==============================] - 40s - loss: 2.7191 - acc: 0.3396    \n",
      "Epoch 2/8\n",
      "7919/7919 [==============================] - 38s - loss: 1.6840 - acc: 0.6862    \n",
      "Epoch 3/8\n",
      "7919/7919 [==============================] - 40s - loss: 0.9258 - acc: 0.8067    \n",
      "Epoch 4/8\n",
      "7919/7919 [==============================] - 40s - loss: 0.5595 - acc: 0.8775    \n",
      "Epoch 5/8\n",
      "7919/7919 [==============================] - 39s - loss: 0.3478 - acc: 0.9261    \n",
      "Epoch 6/8\n",
      "7919/7919 [==============================] - 39s - loss: 0.2140 - acc: 0.9601    \n",
      "Epoch 7/8\n",
      "7919/7919 [==============================] - 40s - loss: 0.1251 - acc: 0.9792    \n",
      "Epoch 8/8\n",
      "7919/7919 [==============================] - 41s - loss: 0.0699 - acc: 0.9904    \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.90      0.87      0.88       151\n",
      "           comp.graphics       0.84      0.80      0.82       196\n",
      " comp.os.ms-windows.misc       0.84      0.81      0.83       161\n",
      "comp.sys.ibm.pc.hardware       0.62      0.80      0.70       167\n",
      "   comp.sys.mac.hardware       0.85      0.84      0.84       176\n",
      "          comp.windows.x       0.88      0.83      0.85       180\n",
      "            misc.forsale       0.75      0.74      0.74       174\n",
      "               rec.autos       0.82      0.92      0.87       178\n",
      "         rec.motorcycles       0.93      0.88      0.90       176\n",
      "      rec.sport.baseball       0.92      0.91      0.91       186\n",
      "        rec.sport.hockey       0.96      0.94      0.95       184\n",
      "               sci.crypt       0.96      0.93      0.95       171\n",
      "         sci.electronics       0.80      0.79      0.79       175\n",
      "                 sci.med       0.90      0.86      0.88       177\n",
      "               sci.space       0.89      0.92      0.91       184\n",
      "  soc.religion.christian       0.81      0.91      0.85       181\n",
      "      talk.politics.guns       0.93      0.91      0.92       159\n",
      "   talk.politics.mideast       0.96      0.92      0.94       153\n",
      "      talk.politics.misc       0.91      0.86      0.89       170\n",
      "      talk.religion.misc       0.81      0.74      0.77        96\n",
      "\n",
      "             avg / total       0.86      0.86      0.86      3395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convolutional model: https://arxiv.org/abs/1408.5882\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout,  Input, Dense, Activation, Flatten\n",
    "from keras.models import Sequential, Model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def model_factory(seq_len, \n",
    "                  vocab_size, \n",
    "                  embedding_size, \n",
    "                  n_classes, \n",
    "                  model_type='sequential',\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['acc'], \n",
    "                  optimizer='rmsprop'):\n",
    "    \n",
    "    def create_sequential_model():\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_size, input_length=seq_len))\n",
    "        model.add(Conv1D(64, 3, strides=1, padding='valid'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(n_classes,  activation='softmax'))\n",
    "        return model\n",
    "        \n",
    "    def create_non_sequential_model():\n",
    "        _input = Input(shape=(seq_len,), dtype='int32')\n",
    "        _embedding = Embedding(vocab_size, embedding_size, input_length=seq_len)(_input)\n",
    "\n",
    "        # each filter is (3 x 300 ) array of weights\n",
    "        # window (kernel_size) is 3\n",
    "        # so number of weights is (3 * 300 * 64)\n",
    "        # each filter outputs a (200 / strides) x 1 transformation\n",
    "        # padding is how we handle boundaries. include + pad, ignore, etc\n",
    "        _conv_1 = Conv1D(64, 3, strides=1, padding='valid')(_embedding)\n",
    "\n",
    "        # Cuts the size of the output in half, maxing over every 2 inputs\n",
    "        _pool_1 = MaxPooling1D(pool_size=2)(_conv_1)\n",
    "        _conv_2 = Conv1D(64, 3, padding='valid')(_pool_1)\n",
    "        _pool_2 = GlobalMaxPooling1D()(_conv_2) \n",
    "        _activation = Activation('relu')(_pool_2)\n",
    "        output = Dense(n_classes,  activation='softmax')(_activation)\n",
    "        model = Model(inputs=_input, outputs=output)\n",
    "        return model\n",
    "        \n",
    "\n",
    "    def create_model():\n",
    "        if model_type=='sequential':\n",
    "            model = create_sequential_model()\n",
    "        elif model_type == 'non-sequential':\n",
    "            model = create_non_sequential_model()        \n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized model type {}\".format(model_type))\n",
    "\n",
    "        model.compile(loss=loss,\n",
    "                     optimizer=optimizer,\n",
    "                     metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    return create_model\n",
    "    \n",
    "seq_len = 350\n",
    "vocab_size = 25000\n",
    "embedding_size = 300\n",
    "epochs = 8\n",
    "batch_size = 100\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "model_build = model_factory(seq_len, vocab_size, embedding_size, n_classes)\n",
    "model2 = KerasClassifier(build_fn=model_build, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "processor = FunctionTransformer(TextProcesser(docs_train, nlp=nlp, max_len=seq_len), validate=False)\n",
    "pipeline2 = make_pipeline(processor, model2)\n",
    "\n",
    "# need to one hot encode y labels\n",
    "y2_train = label_binarize(y_train, classes=range(len(np.unique(dataset.target_names))))\n",
    "pipeline2.fit(docs_train, y2_train)\n",
    "\n",
    "# make model silent after training\n",
    "params = model2.get_params()\n",
    "params = {key: value for key, value in params.items() if key != 'build_fn'}\n",
    "params['verbose'] = 0\n",
    "model2.set_params(**params)\n",
    "\n",
    "# Model Performance on Holdout\n",
    "print(\n",
    "    classification_report(y_test, \n",
    "                          pipeline2.predict(docs_test), \n",
    "                          target_names=dataset.target_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model explanations\n",
    "Here, we'll wrap each pipeline into a Skater model object. We'll use this model object to generate LIME explanations in HTML to help better understand how each model makes predictions. We'll wrap this functionality into an ipywidget to allow the user the (a) modify the text and (b) toggle between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /usr/local/lib/python2.7/dist-packages\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python2.7/dist-packages (from ipywidgets)\n",
      "Requirement already satisfied: ipython<6.0.0,>=4.0.0; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from ipywidgets)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python2.7/dist-packages (from ipywidgets)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python2.7/dist-packages (from ipywidgets)\n",
      "Requirement already satisfied: widgetsnbextension~=3.0.0 in /usr/local/lib/python2.7/dist-packages (from ipywidgets)\n",
      "Requirement already satisfied: jupyter-client in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.5.1->ipywidgets)\n",
      "Requirement already satisfied: tornado>=4.0 in /usr/local/lib/python2.7/dist-packages (from ipykernel>=4.5.1->ipywidgets)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: backports.shutil-get-terminal-size; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: pathlib2; python_version == \"2.7\" or python_version == \"3.3\" in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python2.7/dist-packages (from ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: jupyter-core in /usr/local/lib/python2.7/dist-packages (from nbformat>=4.2.0->ipywidgets)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python2.7/dist-packages (from nbformat>=4.2.0->ipywidgets)\n",
      "Requirement already satisfied: ipython-genutils in /usr/local/lib/python2.7/dist-packages (from nbformat>=4.2.0->ipywidgets)\n",
      "Requirement already satisfied: six in /usr/local/lib/python2.7/dist-packages (from traitlets>=4.3.1->ipywidgets)\n",
      "Requirement already satisfied: enum34; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from traitlets>=4.3.1->ipywidgets)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python2.7/dist-packages (from widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python2.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python2.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets)\n",
      "Requirement already satisfied: singledispatch in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.5.1->ipywidgets)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.5.1->ipywidgets)\n",
      "Requirement already satisfied: backports_abc>=0.4 in /usr/local/lib/python2.7/dist-packages (from tornado>=4.0->ipykernel>=4.5.1->ipywidgets)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python2.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python2.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: scandir; python_version < \"3.5\" in /usr/local/lib/python2.7/dist-packages (from pathlib2; python_version == \"2.7\" or python_version == \"3.3\"->ipython<6.0.0,>=4.0.0; python_version < \"3.3\"->ipywidgets)\n",
      "Requirement already satisfied: functools32; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from jsonschema!=2.5.0,>=2.4->nbformat>=4.2.0->ipywidgets)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python2.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python2.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: nbconvert in /usr/local/lib/python2.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python2.7/dist-packages (from jinja2->notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python2.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: testpath in /usr/local/lib/python2.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python2.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: mistune>=0.7.4 in /usr/local/lib/python2.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: bleach in /usr/local/lib/python2.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: configparser>=3.5; python_version == \"2.7\" in /usr/local/lib/python2.7/dist-packages (from entrypoints>=0.2.2->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Requirement already satisfied: html5lib!=0.9999,!=0.99999,<0.99999999,>=0.999 in /usr/local/lib/python2.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.0.0->ipywidgets)\n",
      "Enabling notebook extension jupyter-js-widgets/extension...\n",
      "      - Validating: \u001b[32mOK\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## You may need to enable ipywidgets\n",
    "!sudo pip install ipywidgets\n",
    "!jupyter nbextension enable --py --user widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting skater\n",
      "  Downloading skater-1.0.2.tar.gz\n",
      "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python2.7/dist-packages (from skater)\n",
      "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python2.7/dist-packages (from skater)\n",
      "Collecting ds-lime>=0.1.1.21 (from skater)\n",
      "  Downloading ds-lime-0.1.1.27.tar.gz (253kB)\n",
      "\u001b[K    100% |################################| 256kB 3.0MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python2.7/dist-packages (from skater)\n",
      "Collecting pathos==0.2.0 (from skater)\n",
      "  Downloading pathos-0.2.0.tgz (68kB)\n",
      "\u001b[K    100% |################################| 71kB 4.4MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: dill>=0.2.6 in /usr/local/lib/python2.7/dist-packages (from skater)\n",
      "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python2.7/dist-packages (from pandas>=0.19->skater)\n",
      "Requirement already satisfied: numpy>=1.7.0 in /usr/local/lib/python2.7/dist-packages (from pandas>=0.19->skater)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python2.7/dist-packages (from pandas>=0.19->skater)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python2.7/dist-packages (from ds-lime>=0.1.1.21->skater)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python2.7/dist-packages (from requests->skater)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python2.7/dist-packages (from requests->skater)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python2.7/dist-packages (from requests->skater)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python2.7/dist-packages (from requests->skater)\n",
      "Collecting ppft>=1.6.4.5 (from pathos==0.2.0->skater)\n",
      "  Downloading ppft-1.6.4.7.1.zip (78kB)\n",
      "\u001b[K    100% |################################| 81kB 4.7MB/s ta 0:00:011\n",
      "\u001b[?25hCollecting pox>=0.2.2 (from pathos==0.2.0->skater)\n",
      "  Downloading pox-0.2.3.zip (41kB)\n",
      "\u001b[K    100% |################################| 51kB 8.7MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting multiprocess>=0.70.4 (from pathos==0.2.0->skater)\n",
      "  Downloading multiprocess-0.70.5.zip (1.5MB)\n",
      "\u001b[K    100% |################################| 1.5MB 872kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python2.7/dist-packages (from python-dateutil->pandas>=0.19->skater)\n",
      "Building wheels for collected packages: skater, ds-lime, pathos, ppft, pox, multiprocess\n",
      "  Running setup.py bdist_wheel for skater ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/2b/80/c4/27748f133a062c246ef28f8916ed8c7bd7f4dc040f3d894be0\n",
      "  Running setup.py bdist_wheel for ds-lime ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/a8/fe/5a/7a7bfca3a84f80ad75d6885ee6e78a887e0b0b480bb2ff0cc6\n",
      "  Running setup.py bdist_wheel for pathos ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ac/dd/95/a910070ff4be5079956b2764f61bb8b05e37c7e7563b5e7848\n",
      "  Running setup.py bdist_wheel for ppft ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/4d/28/cc/c5bca1e1d3923a5f4dd50219a9e41391bbeb832835e7905344\n",
      "  Running setup.py bdist_wheel for pox ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/ff/b6/98/d03a0a0f153267d1601c3cbaa888ec4cdd52d658a945d44f15\n",
      "  Running setup.py bdist_wheel for multiprocess ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/28/ef/9f/5cc70b5d92fc4641b68dc23b3583f2b6ec1d153cb71985aeaf\n",
      "Successfully built skater ds-lime pathos ppft pox multiprocess\n",
      "Installing collected packages: ds-lime, ppft, pox, multiprocess, pathos, skater\n",
      "Successfully installed ds-lime-0.1.1.27 multiprocess-0.70.5 pathos-0.2.0 pox-0.2.3 ppft-1.6.4.7.1 skater-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!sudo pip install skater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create the explorer app.\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from ipywidgets import Button, Textarea, Layout, Box, Label, Text, Output, RadioButtons, HBox\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n",
    "\n",
    "class TextExplainer(object):\n",
    "    def __init__(self, models, init_pattern=\"\"):\n",
    "        \"\"\"\n",
    "        Display box for LIME results.\n",
    "        \n",
    "        models: dictionary of skater of models.\n",
    "            Keys correspond to user-defined model names, used for radio buttons.\n",
    "            Values are skater models used to generate predictions.\n",
    "    \n",
    "        \"\"\"\n",
    "        self.status = \"Ready\"\n",
    "        self.explainer = LimeTextExplainer(class_names=dataset.target_names)\n",
    "        self.models = models\n",
    "        self.model_names = list(self.models.keys())\n",
    "        self.text_field = Textarea(init_pattern, layout=Layout(height='200px', width='500px'))\n",
    "        self.text_box = Box([Label(value='Text Box'), self.text_field])\n",
    "        \n",
    "        self.status_field = Label(self.status, layout=Layout(height='50px', width='100px'))        \n",
    "        self.status_box = Box([Label(value='Status'), self.status_field])\n",
    "\n",
    "        self.match_button = Button(description='Explain', )\n",
    "        self.match_button.on_click(self.match_pattern)\n",
    "        \n",
    "        self.model_selectors = RadioButtons(\n",
    "            options = self.model_names,\n",
    "            description = \"Use Model\"\n",
    "        )\n",
    "        \n",
    "        self.inputs_box = HBox([self.text_box, self.model_selectors])        \n",
    "        \n",
    "        self.explanation_area = Output()\n",
    "        display(self.inputs_box)       \n",
    "        display(self.match_button)\n",
    "        display(self.status_box)\n",
    "        display(self.explanation_area)\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.models[self.model_selectors.value]\n",
    "            \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self.text_field.value    \n",
    "    \n",
    "    def match_pattern(self, b):\n",
    "        self.status_field.value = 'loading'\n",
    "        with self.explanation_area:\n",
    "            clear_output()\n",
    "            display(HTML(self.get_explanation_as_html(self.text)))\n",
    "        self.status_field.value = 'ready'\n",
    "\n",
    "    def get_explanation_as_html(self, text):\n",
    "        \n",
    "        # generate most likely class to confine LIME results\n",
    "\n",
    "        explanation = self.explainer.explain_instance(text, \n",
    "                                                      self.model, \n",
    "                                                      top_labels=1)\n",
    "\n",
    "        return explanation.as_html()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "585bd52462ed41d3a7383a5dd239e5c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d2e292a5e4945b19f8781e7e265862f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9654bd1710a4106a4e3e987edd5305e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfb4c089049949a9a1978f4c405250db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = {\"CNN\": pipeline2.predict_proba, ' GBC-Pretrain': pipeline.predict_proba}\n",
    "r = TextExplainer(models, docs_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
