{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring NLP Models with Skater/LIME\n",
    "\n",
    "In this example, we'll train a couple types of models, and use Skater, LIME, and ipywidgets to interactively explore model behavior.\n",
    "\n",
    "### Install Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!conda install -y tensorflow\n",
    "!conda install -y keras\n",
    "!conda install -y spacy\n",
    "!python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SpaCy Language Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#gimme data\n",
    "dataset = fetch_20newsgroups()\n",
    "docs = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(docs, y, test_size = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Pretrained Word Embeddings\n",
    "\n",
    "We will use SpaCy's pretrained word embeddings as document representations, and feed these representations into a gradient boosting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gimme vectors\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from spacy.tokens.doc import Doc\n",
    "from sklearn.metrics import classification_report\n",
    "import six\n",
    "def doc2vec(x):\n",
    "    if isinstance(x, (six.binary_type, six.string_types)):\n",
    "        return nlp(x, parse = False, entity = False, tag = False).vector\n",
    "    \n",
    "    elif type(x) in [list, tuple, np.ndarray]:\n",
    "        return np.array([doc2vec(six.text_type(doc)) for doc in x])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized Input\") \n",
    "\n",
    "# build a pipeline of text -> vector (transformer), vector -> predictions (model)\n",
    "model = GradientBoostingClassifier(n_estimators = 50)\n",
    "transformer = FunctionTransformer(func = doc2vec, validate=False)\n",
    "pipeline = make_pipeline(transformer, model)\n",
    "pipeline.fit(docs_train, y_train)       \n",
    "\n",
    "#Classification Report on Holdout\n",
    "print(\n",
    "    classification_report(y_test, \n",
    "                          pipeline.predict(docs_test), \n",
    "                          target_names=dataset.target_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(docs_train[:10], y_train[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: CNN \n",
    "In this model, we convert text to a list of padded lists of word IDs, to be used in an embedding lookup table. The embeddings will be trained as part of a CNN implemented with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, corpus, nlp=None, max_len=200, max_vocab_size=20000):\n",
    "        \"\"\"\n",
    "        corpus: list of strings\n",
    "            Documents used to initialize vocabulary.\n",
    "            \n",
    "        nlp: Spacy language model\n",
    "            If none then will build one in __init__\n",
    "            \n",
    "        max_len: int\n",
    "            Maximum length of a document sequence. Balance information with scale of data.\n",
    "            \n",
    "        max_vocab_size: int\n",
    "        \n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        self.PADDING_VAL = 1\n",
    "        self.MISSING_VAL = 2\n",
    "        self.START_VAL = 3\n",
    "        self.END_VAL = 4\n",
    "        self.vocab = {}\n",
    "        self.vocab_counts = Counter()#Counter(['PADDING_VAL','MISSING_VAL','START_VAL','END_VAL'])\n",
    "        self.build_vocab(corpus)\n",
    "\n",
    "        \n",
    "    def pad(self, obj):\n",
    "        n_pads = max(self.max_len - len(obj) - 2, 0)\n",
    "        we_can_take = self.max_len - 2\n",
    "        result = [self.START_VAL] + obj[:we_can_take] + [self.END_VAL] + [self.PADDING_VAL] * n_pads\n",
    "        return result\n",
    "        \n",
    "    def get_current_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def update(self, words):\n",
    "        for word in words:\n",
    "            self.vocab_counts.update([word])\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        self.vocab = {}\n",
    "        self.vocab_counts = Counter()\n",
    "        \n",
    "        for doc in nlp.tokenizer.pipe(map(six.text_type, corpus)):\n",
    "            self.update(map(self._process_token, doc))\n",
    "            \n",
    "        for i, (word, count) in enumerate(self.vocab_counts.most_common(self.max_vocab_size)):\n",
    "            self.vocab[word] = i\n",
    "        \n",
    "    def _process_token(self, token):\n",
    "        if token.is_space:\n",
    "            return \"SPACE\"\n",
    "        elif token.is_punct:\n",
    "            return \"PUNCT\"       \n",
    "        elif token.like_url:\n",
    "            return \"URL\"\n",
    "        elif token.like_email:\n",
    "            return \"EMAIL\"\n",
    "        elif token.like_num:\n",
    "            return \"NUM\"\n",
    "        else:\n",
    "            return token.lower_\n",
    "\n",
    "    def process_token(self, token):\n",
    "        return self.vocab.get(self._process_token(token), self.MISSING_VAL)\n",
    "\n",
    "    def process(self, texts):\n",
    "        docs = []\n",
    "        for doc in self.nlp.tokenizer.pipe(list(texts)):\n",
    "            docs.append(self.pad(list(map(self.process_token, doc))))\n",
    "        return np.array(docs)\n",
    "            \n",
    "    def __call__(self, texts):\n",
    "        return self.process(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolutional model: https://arxiv.org/abs/1408.5882\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout,  Input, Dense, Activation, Flatten\n",
    "from keras.models import Sequential, Model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def model_factory(seq_len, \n",
    "                  vocab_size, \n",
    "                  embedding_size, \n",
    "                  n_classes, \n",
    "                  model_type='sequential',\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['acc'], \n",
    "                  optimizer='rmsprop'):\n",
    "    \n",
    "    def create_sequential_model():\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_size, input_length=seq_len))\n",
    "        model.add(Conv1D(64, 3, strides=1, padding='valid'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(n_classes,  activation='softmax'))\n",
    "        return model\n",
    "        \n",
    "    def create_non_sequential_model():\n",
    "        _input = Input(shape=(seq_len,), dtype='int32')\n",
    "        _embedding = Embedding(vocab_size, embedding_size, input_length=seq_len)(_input)\n",
    "\n",
    "        # each filter is (3 x 300 ) array of weights\n",
    "        # window (kernel_size) is 3\n",
    "        # so number of weights is (3 * 300 * 64)\n",
    "        # each filter outputs a (200 / strides) x 1 transformation\n",
    "        # padding is how we handle boundaries. include + pad, ignore, etc\n",
    "        _conv_1 = Conv1D(64, 3, strides=1, padding='valid')(_embedding)\n",
    "\n",
    "        # Cuts the size of the output in half, maxing over every 2 inputs\n",
    "        _pool_1 = MaxPooling1D(pool_size=2)(_conv_1)\n",
    "        _conv_2 = Conv1D(64, 3, padding='valid')(_pool_1)\n",
    "        _pool_2 = GlobalMaxPooling1D()(_conv_2) \n",
    "        _activation = Activation('relu')(_pool_2)\n",
    "        output = Dense(n_classes,  activation='softmax')(_activation)\n",
    "        model = Model(inputs=_input, outputs=output)\n",
    "        return model\n",
    "        \n",
    "\n",
    "    def create_model():\n",
    "        if model_type=='sequential':\n",
    "            model = create_sequential_model()\n",
    "        elif model_type == 'non-sequential':\n",
    "            model = create_non_sequential_model()        \n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized model type {}\".format(model_type))\n",
    "\n",
    "        model.compile(loss=loss,\n",
    "                     optimizer=optimizer,\n",
    "                     metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    return create_model\n",
    "    \n",
    "seq_len = 350\n",
    "vocab_size = 25000\n",
    "embedding_size = 300\n",
    "epochs = 8\n",
    "batch_size = 100\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "model_build = model_factory(seq_len, vocab_size, embedding_size, n_classes)\n",
    "model2 = KerasClassifier(build_fn=model_build, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "processor = FunctionTransformer(TextProcesser(docs_train, nlp=nlp, max_len=seq_len), validate=False)\n",
    "pipeline2 = make_pipeline(processor, model2)\n",
    "\n",
    "# need to one hot encode y labels\n",
    "y2_train = label_binarize(y_train, classes=range(len(np.unique(dataset.target_names))))\n",
    "pipeline2.fit(docs_train, y2_train)\n",
    "\n",
    "# make model silent after training\n",
    "params = model2.get_params()\n",
    "params = {key: value for key, value in params.items() if key != 'build_fn'}\n",
    "params['verbose'] = 0\n",
    "model2.set_params(**params)\n",
    "\n",
    "# Model Performance on Holdout\n",
    "print(\n",
    "    classification_report(y_test, \n",
    "                          pipeline2.predict(docs_test), \n",
    "                          target_names=dataset.target_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model explanations\n",
    "Here, we'll wrap each pipeline into a Skater model object. We'll use this model object to generate LIME explanations in HTML to help better understand how each model makes predictions. We'll wrap this functionality into an ipywidget to allow the user the (a) modify the text and (b) toggle between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create the explorer app.\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from ipywidgets import Button, Textarea, Layout, Box, Label, Text, Output, RadioButtons, HBox\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n",
    "\n",
    "class TextExplainer(object):\n",
    "    def __init__(self, models, init_pattern=\"\"):\n",
    "        \"\"\"\n",
    "        Display box for LIME results.\n",
    "        \n",
    "        models: dictionary of skater of models.\n",
    "            Keys correspond to user-defined model names, used for radio buttons.\n",
    "            Values are skater models used to generate predictions.\n",
    "    \n",
    "        \"\"\"\n",
    "        self.status = \"Ready\"\n",
    "        self.explainer = LimeTextExplainer(class_names=dataset.target_names)\n",
    "        self.models = models\n",
    "        self.model_names = list(self.models.keys())\n",
    "        self.text_field = Textarea(init_pattern, layout=Layout(height='200px', width='500px'))\n",
    "        self.text_box = Box([Label(value='Text Box'), self.text_field])\n",
    "        \n",
    "        self.status_field = Label(self.status, layout=Layout(height='50px', width='100px'))        \n",
    "        self.status_box = Box([Label(value='Status'), self.status_field])\n",
    "\n",
    "        self.match_button = Button(description='Explain', )\n",
    "        self.match_button.on_click(self.match_pattern)\n",
    "        \n",
    "        self.model_selectors = RadioButtons(\n",
    "            options = self.model_names,\n",
    "            description = \"Use Model\"\n",
    "        )\n",
    "        \n",
    "        self.inputs_box = HBox([self.text_box, self.model_selectors])        \n",
    "        \n",
    "        self.explanation_area = Output()\n",
    "        display(self.inputs_box)       \n",
    "        display(self.match_button)\n",
    "        display(self.status_box)\n",
    "        display(self.explanation_area)\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.models[self.model_selectors.value]\n",
    "            \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self.text_field.value    \n",
    "    \n",
    "    def match_pattern(self, b):\n",
    "        self.status_field.value = 'loading'\n",
    "        with self.explanation_area:\n",
    "            clear_output()\n",
    "            display(HTML(self.get_explanation_as_html(self.text)))\n",
    "        self.status_field.value = 'ready'\n",
    "\n",
    "    def get_explanation_as_html(self, text):\n",
    "        \n",
    "        # generate most likely class to confine LIME results\n",
    "        explanation = self.explainer.explain_instance(text, \n",
    "                                                      self.model, \n",
    "                                                      top_labels=1)\n",
    "\n",
    "        return explanation.as_html()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\"CNN\": pipeline2.predict_proba, ' GBC-Pretrain': pipeline.predict_proba}\n",
    "r = TextExplainer(models, docs_test[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
