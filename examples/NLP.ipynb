{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring NLP Models with Skater/LIME\n",
    "\n",
    "In this example, we'll train a couple types of models, and use Skater, LIME, and ipywidgets to interactively explore model behavior.\n",
    "\n",
    "### Install Deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting np_utils\n",
      "  Downloading np_utils-0.5.3.4.tar.gz (56kB)\n",
      "\u001b[K    100% |████████████████████████████████| 61kB 329kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.0 (from np_utils)\n",
      "  Downloading numpy-1.13.1-cp36-cp36m-manylinux1_x86_64.whl (17.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.0MB 43kB/s eta 0:00:01  2% |█                               | 491kB 2.0MB/s eta 0:00:09    5% |█▋                              | 870kB 2.2MB/s eta 0:00:08    28% |█████████                       | 4.8MB 2.3MB/s eta 0:00:06    45% |██████████████▌                 | 7.7MB 1.2MB/s eta 0:00:08    51% |████████████████▋               | 8.8MB 746kB/s eta 0:00:11    58% |██████████████████▉             | 10.0MB 1.4MB/s eta 0:00:05    60% |███████████████████▎            | 10.2MB 1.1MB/s eta 0:00:07    75% |████████████████████████▏       | 12.8MB 1.7MB/s eta 0:00:03    93% |█████████████████████████████▉  | 15.8MB 1.9MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting future>=0.16 (from np_utils)\n",
      "  Downloading future-0.16.0.tar.gz (824kB)\n",
      "\u001b[K    100% |████████████████████████████████| 829kB 572kB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: np-utils, future\n",
      "  Running setup.py bdist_wheel for np-utils ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/5d/be/b5/e223535ec3efb733df6afc2518d90d398bbe759e665683b025\n",
      "  Running setup.py bdist_wheel for future ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/c2/50/7c/0d83b4baac4f63ff7a765bd16390d2ab43c93587fac9d6017a\n",
      "Successfully built np-utils future\n",
      "Installing collected packages: numpy, future, np-utils\n",
      "  Found existing installation: numpy 1.12.1\n",
      "    Uninstalling numpy-1.12.1:\n",
      "      Successfully uninstalled numpy-1.12.1\n",
      "Successfully installed future-0.16.0 np-utils-0.5.3.4 numpy-1.13.1\n",
      "Requirement already up-to-date: theano in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already up-to-date: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from theano)\n",
      "Requirement already up-to-date: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from theano)\n",
      "Collecting six>=1.9.0 (from theano)\n",
      "  Downloading six-1.11.0-py2.py3-none-any.whl\n",
      "Installing collected packages: six\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "Successfully installed six-1.11.0\n",
      "Requirement already up-to-date: tensorflow in /opt/conda/lib/python3.6/site-packages\n",
      "Collecting tensorflow-tensorboard<0.2.0,>=0.1.0 (from tensorflow)\n",
      "  Downloading tensorflow_tensorboard-0.1.6-py3-none-any.whl (2.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.2MB 302kB/s ta 0:00:01    53% |█████████████████               | 1.2MB 1.5MB/s eta 0:00:01    56% |██████████████████▎             | 1.2MB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wheel>=0.26 (from tensorflow)\n",
      "  Downloading wheel-0.30.0-py2.py3-none-any.whl (49kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 689kB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already up-to-date: six>=1.10.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow)\n",
      "Collecting protobuf>=3.3.0 (from tensorflow)\n",
      "  Downloading protobuf-3.4.0-cp36-cp36m-manylinux1_x86_64.whl (6.2MB)\n",
      "\u001b[K    100% |████████████████████████████████| 6.2MB 116kB/s ta 0:00:011   14% |████▊                           | 911kB 2.1MB/s eta 0:00:03    84% |███████████████████████████     | 5.2MB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: numpy>=1.11.0 in /opt/conda/lib/python3.6/site-packages (from tensorflow)\n",
      "Requirement already up-to-date: werkzeug>=0.11.10 in /opt/conda/lib/python3.6/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Collecting html5lib==0.9999999 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "  Downloading html5lib-0.9999999.tar.gz (889kB)\n",
      "\u001b[K    100% |████████████████████████████████| 890kB 864kB/s ta 0:00:01\n",
      "\u001b[?25hCollecting bleach==1.5.0 (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "  Downloading bleach-1.5.0-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: markdown>=2.6.8 in /opt/conda/lib/python3.6/site-packages (from tensorflow-tensorboard<0.2.0,>=0.1.0->tensorflow)\n",
      "Collecting setuptools (from protobuf>=3.3.0->tensorflow)\n",
      "  Downloading setuptools-36.5.0-py2.py3-none-any.whl (478kB)\n",
      "\u001b[K    100% |████████████████████████████████| 481kB 1.5MB/s ta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: html5lib\n",
      "  Running setup.py bdist_wheel for html5lib ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/6f/85/6c/56b8e1292c6214c4eb73b9dda50f53e8e977bf65989373c962\n",
      "Successfully built html5lib\n",
      "Installing collected packages: setuptools, protobuf, wheel, html5lib, bleach, tensorflow-tensorboard\n",
      "  Found existing installation: setuptools 36.2.2.post20170724\n",
      "    Uninstalling setuptools-36.2.2.post20170724:\n",
      "      Successfully uninstalled setuptools-36.2.2.post20170724\n",
      "  Found existing installation: protobuf 3.3.2\n",
      "    Uninstalling protobuf-3.3.2:\n",
      "      Successfully uninstalled protobuf-3.3.2\n",
      "  Found existing installation: wheel 0.29.0\n",
      "    Uninstalling wheel-0.29.0:\n",
      "      Successfully uninstalled wheel-0.29.0\n",
      "  Found existing installation: html5lib 0.999999999\n",
      "    Uninstalling html5lib-0.999999999:\n",
      "      Successfully uninstalled html5lib-0.999999999\n",
      "  Found existing installation: bleach 2.0.0\n",
      "    Uninstalling bleach-2.0.0:\n",
      "      Successfully uninstalled bleach-2.0.0\n",
      "Successfully installed bleach-1.5.0 html5lib-0.9999999 protobuf-3.4.0 setuptools-36.5.0 tensorflow-tensorboard-0.1.6 wheel-0.30.0\n",
      "Requirement already satisfied: keras==2.0.6 in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: theano in /opt/conda/lib/python3.6/site-packages (from keras==2.0.6)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from keras==2.0.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from keras==2.0.6)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /opt/conda/lib/python3.6/site-packages (from theano->keras==2.0.6)\n",
      "Requirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from theano->keras==2.0.6)\n",
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: numpy>=1.7 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: murmurhash<0.27,>=0.26 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: cymem<1.32,>=1.30 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: thinc<6.6.0,>=6.5.0 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: pip<10.0.0,>=9.0.0 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: pathlib in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: ujson>=1.35 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: dill<0.3,>=0.2 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: regex<2017.12.1,>=2017.4.1 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: ftfy<5.0.0,>=4.4.2 in /opt/conda/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already satisfied: wrapt in /opt/conda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /opt/conda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "Requirement already satisfied: cytoolz<0.9,>=0.8 in /opt/conda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.6/site-packages (from thinc<6.6.0,>=6.5.0->spacy)\n",
      "Requirement already satisfied: html5lib in /opt/conda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.6/site-packages (from ftfy<5.0.0,>=4.4.2->spacy)\n",
      "Requirement already satisfied: toolz>=0.8.0 in /opt/conda/lib/python3.6/site-packages (from cytoolz<0.9,>=0.8->thinc<6.6.0,>=6.5.0->spacy)\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade np_utils\n",
    "! pip install --upgrade theano\n",
    "! pip install --upgrade tensorflow\n",
    "! pip install keras==2.0.6\n",
    "! pip install spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Restart kernel\n",
    "from __future__ import absolute_import\n",
    "\n",
    "! python -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SpaCy Language Model and Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import warnings\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "warnings.filterwarnings('ignore')\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#gimme data\n",
    "dataset = fetch_20newsgroups()\n",
    "docs = dataset.data\n",
    "y = dataset.target\n",
    "\n",
    "docs_train, docs_test, y_train, y_test = train_test_split(docs, y, test_size = .3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1: Pretrained Word Embeddings\n",
    "\n",
    "We will use SpaCy's pretrained word embeddings as document representations, and feed these representations into a gradient boosting classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.59      0.46      0.52       151\n",
      "           comp.graphics       0.62      0.52      0.56       196\n",
      " comp.os.ms-windows.misc       0.56      0.63      0.59       161\n",
      "comp.sys.ibm.pc.hardware       0.48      0.48      0.48       167\n",
      "   comp.sys.mac.hardware       0.62      0.38      0.47       176\n",
      "          comp.windows.x       0.58      0.71      0.64       180\n",
      "            misc.forsale       0.65      0.78      0.71       174\n",
      "               rec.autos       0.68      0.60      0.64       178\n",
      "         rec.motorcycles       0.60      0.72      0.65       176\n",
      "      rec.sport.baseball       0.63      0.66      0.65       186\n",
      "        rec.sport.hockey       0.74      0.79      0.76       184\n",
      "               sci.crypt       0.70      0.82      0.76       171\n",
      "         sci.electronics       0.62      0.58      0.60       175\n",
      "                 sci.med       0.75      0.86      0.81       177\n",
      "               sci.space       0.69      0.73      0.71       184\n",
      "  soc.religion.christian       0.56      0.81      0.66       181\n",
      "      talk.politics.guns       0.63      0.64      0.64       159\n",
      "   talk.politics.mideast       0.52      0.70      0.59       153\n",
      "      talk.politics.misc       0.80      0.35      0.48       170\n",
      "      talk.religion.misc       0.60      0.03      0.06        96\n",
      "\n",
      "             avg / total       0.63      0.63      0.61      3395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#gimme vectors\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from spacy.tokens.doc import Doc\n",
    "from sklearn.metrics import classification_report\n",
    "import six\n",
    "def doc2vec(x):\n",
    "    if isinstance(x, (six.binary_type, six.string_types)):\n",
    "        return nlp(x, parse = False, entity = False, tag = False).vector\n",
    "    \n",
    "    elif type(x) in [list, tuple, np.ndarray]:\n",
    "        return np.array([doc2vec(six.text_type(doc)) for doc in x])\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Unrecognized Input\") \n",
    "\n",
    "# build a pipeline of text -> vector (transformer), vector -> predictions (model)\n",
    "model = GradientBoostingClassifier(n_estimators = 50)\n",
    "model = LogisticRegression()\n",
    "\n",
    "transformer = FunctionTransformer(func = doc2vec, validate=False)\n",
    "pipeline = make_pipeline(transformer, model)\n",
    "pipeline.fit(docs_train, y_train)       \n",
    "\n",
    "#Classification Report on Holdout\n",
    "print(\n",
    "    classification_report(y_test, \n",
    "                          pipeline.predict(docs_test), \n",
    "                          target_names=dataset.target_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2: CNN \n",
    "In this model, we convert text to a list of padded lists of word IDs, to be used in an embedding lookup table. The embeddings will be trained as part of a CNN implemented with Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.attrs import ORTH, IS_PUNCT\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "class TextProcesser(object):\n",
    "    def __init__(self, corpus, nlp=None, max_len=200, max_vocab_size=20000):\n",
    "        \"\"\"\n",
    "        corpus: list of strings\n",
    "            Documents used to initialize vocabulary.\n",
    "            \n",
    "        nlp: Spacy language model\n",
    "            If none then will build one in __init__\n",
    "            \n",
    "        max_len: int\n",
    "            Maximum length of a document sequence. Balance information with scale of data.\n",
    "            \n",
    "        max_vocab_size: int\n",
    "        \n",
    "        \"\"\"\n",
    "        self.max_vocab_size = max_vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.nlp = nlp or spacy.load('en')\n",
    "        self.PADDING_VAL = 1\n",
    "        self.MISSING_VAL = 2\n",
    "        self.START_VAL = 3\n",
    "        self.END_VAL = 4\n",
    "        self.vocab = {}\n",
    "        self.vocab_counts = Counter()#Counter(['PADDING_VAL','MISSING_VAL','START_VAL','END_VAL'])\n",
    "        self.build_vocab(corpus)\n",
    "\n",
    "        \n",
    "    def pad(self, obj):\n",
    "        n_pads = max(self.max_len - len(obj) - 2, 0)\n",
    "        we_can_take = self.max_len - 2\n",
    "        result = [self.START_VAL] + obj[:we_can_take] + [self.END_VAL] + [self.PADDING_VAL] * n_pads\n",
    "        return result\n",
    "        \n",
    "    def get_current_vocab_size(self):\n",
    "        return len(self.vocab)\n",
    "        \n",
    "    def update(self, words):\n",
    "        for word in words:\n",
    "            self.vocab_counts.update([word])\n",
    "\n",
    "    def build_vocab(self, corpus):\n",
    "        self.vocab = {}\n",
    "        self.vocab_counts = Counter()\n",
    "        \n",
    "        for doc in nlp.tokenizer.pipe(map(six.text_type, corpus)):\n",
    "            self.update(map(self._process_token, doc))\n",
    "            \n",
    "        for i, (word, count) in enumerate(self.vocab_counts.most_common(self.max_vocab_size)):\n",
    "            self.vocab[word] = i\n",
    "        \n",
    "    def _process_token(self, token):\n",
    "        if token.is_space:\n",
    "            return \"SPACE\"\n",
    "        elif token.is_punct:\n",
    "            return \"PUNCT\"       \n",
    "        elif token.like_url:\n",
    "            return \"URL\"\n",
    "        elif token.like_email:\n",
    "            return \"EMAIL\"\n",
    "        elif token.like_num:\n",
    "            return \"NUM\"\n",
    "        else:\n",
    "            return token.lower_\n",
    "\n",
    "    def process_token(self, token):\n",
    "        return self.vocab.get(self._process_token(token), self.MISSING_VAL)\n",
    "\n",
    "    def process(self, texts):\n",
    "        docs = []\n",
    "        for doc in self.nlp.tokenizer.pipe(list(texts)):\n",
    "            docs.append(self.pad(list(map(self.process_token, doc))))\n",
    "        return np.array(docs)\n",
    "            \n",
    "    def __call__(self, texts):\n",
    "        return self.process(texts)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "7919/7919 [==============================] - 40s - loss: 2.7191 - acc: 0.3396    \n",
      "Epoch 2/8\n",
      "7919/7919 [==============================] - 38s - loss: 1.6840 - acc: 0.6862    \n",
      "Epoch 3/8\n",
      "7919/7919 [==============================] - 40s - loss: 0.9258 - acc: 0.8067    \n",
      "Epoch 4/8\n",
      "7919/7919 [==============================] - 40s - loss: 0.5595 - acc: 0.8775    \n",
      "Epoch 5/8\n",
      "7919/7919 [==============================] - 39s - loss: 0.3478 - acc: 0.9261    \n",
      "Epoch 6/8\n",
      "7919/7919 [==============================] - 39s - loss: 0.2140 - acc: 0.9601    \n",
      "Epoch 7/8\n",
      "7919/7919 [==============================] - 40s - loss: 0.1251 - acc: 0.9792    \n",
      "Epoch 8/8\n",
      "7919/7919 [==============================] - 41s - loss: 0.0699 - acc: 0.9904    \n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism       0.90      0.87      0.88       151\n",
      "           comp.graphics       0.84      0.80      0.82       196\n",
      " comp.os.ms-windows.misc       0.84      0.81      0.83       161\n",
      "comp.sys.ibm.pc.hardware       0.62      0.80      0.70       167\n",
      "   comp.sys.mac.hardware       0.85      0.84      0.84       176\n",
      "          comp.windows.x       0.88      0.83      0.85       180\n",
      "            misc.forsale       0.75      0.74      0.74       174\n",
      "               rec.autos       0.82      0.92      0.87       178\n",
      "         rec.motorcycles       0.93      0.88      0.90       176\n",
      "      rec.sport.baseball       0.92      0.91      0.91       186\n",
      "        rec.sport.hockey       0.96      0.94      0.95       184\n",
      "               sci.crypt       0.96      0.93      0.95       171\n",
      "         sci.electronics       0.80      0.79      0.79       175\n",
      "                 sci.med       0.90      0.86      0.88       177\n",
      "               sci.space       0.89      0.92      0.91       184\n",
      "  soc.religion.christian       0.81      0.91      0.85       181\n",
      "      talk.politics.guns       0.93      0.91      0.92       159\n",
      "   talk.politics.mideast       0.96      0.92      0.94       153\n",
      "      talk.politics.misc       0.91      0.86      0.89       170\n",
      "      talk.religion.misc       0.81      0.74      0.77        96\n",
      "\n",
      "             avg / total       0.86      0.86      0.86      3395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#convolutional model: https://arxiv.org/abs/1408.5882\n",
    "from keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D, Dropout,  Input, Dense, Activation, Flatten\n",
    "from keras.models import Sequential, Model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "\n",
    "def model_factory(seq_len, \n",
    "                  vocab_size, \n",
    "                  embedding_size, \n",
    "                  n_classes, \n",
    "                  model_type='sequential',\n",
    "                  loss='categorical_crossentropy', \n",
    "                  metrics=['acc'], \n",
    "                  optimizer='rmsprop'):\n",
    "    \n",
    "    def create_sequential_model():\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(vocab_size, embedding_size, input_length=seq_len))\n",
    "        model.add(Conv1D(64, 3, strides=1, padding='valid'))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        model.add(GlobalMaxPooling1D())\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(Dense(n_classes,  activation='softmax'))\n",
    "        return model\n",
    "        \n",
    "    def create_non_sequential_model():\n",
    "        _input = Input(shape=(seq_len,), dtype='int32')\n",
    "        _embedding = Embedding(vocab_size, embedding_size, input_length=seq_len)(_input)\n",
    "\n",
    "        # each filter is (3 x 300 ) array of weights\n",
    "        # window (kernel_size) is 3\n",
    "        # so number of weights is (3 * 300 * 64)\n",
    "        # each filter outputs a (200 / strides) x 1 transformation\n",
    "        # padding is how we handle boundaries. include + pad, ignore, etc\n",
    "        _conv_1 = Conv1D(64, 3, strides=1, padding='valid')(_embedding)\n",
    "\n",
    "        # Cuts the size of the output in half, maxing over every 2 inputs\n",
    "        _pool_1 = MaxPooling1D(pool_size=2)(_conv_1)\n",
    "        _conv_2 = Conv1D(64, 3, padding='valid')(_pool_1)\n",
    "        _pool_2 = GlobalMaxPooling1D()(_conv_2) \n",
    "        _activation = Activation('relu')(_pool_2)\n",
    "        output = Dense(n_classes,  activation='softmax')(_activation)\n",
    "        model = Model(inputs=_input, outputs=output)\n",
    "        return model\n",
    "        \n",
    "\n",
    "    def create_model():\n",
    "        if model_type=='sequential':\n",
    "            model = create_sequential_model()\n",
    "        elif model_type == 'non-sequential':\n",
    "            model = create_non_sequential_model()        \n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized model type {}\".format(model_type))\n",
    "\n",
    "        model.compile(loss=loss,\n",
    "                     optimizer=optimizer,\n",
    "                     metrics=metrics)\n",
    "        return model\n",
    "\n",
    "    return create_model\n",
    "    \n",
    "seq_len = 350\n",
    "vocab_size = 25000\n",
    "embedding_size = 300\n",
    "epochs = 8\n",
    "batch_size = 100\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "model_build = model_factory(seq_len, vocab_size, embedding_size, n_classes)\n",
    "model2 = KerasClassifier(build_fn=model_build, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "processor = FunctionTransformer(TextProcesser(docs_train, nlp=nlp, max_len=seq_len), validate=False)\n",
    "pipeline2 = make_pipeline(processor, model2)\n",
    "\n",
    "# need to one hot encode y labels\n",
    "y2_train = label_binarize(y_train, classes=range(len(np.unique(dataset.target_names))))\n",
    "pipeline2.fit(docs_train, y2_train)\n",
    "\n",
    "# make model silent after training\n",
    "params = model2.get_params()\n",
    "params = {key: value for key, value in params.items() if key != 'build_fn'}\n",
    "params['verbose'] = 0\n",
    "model2.set_params(**params)\n",
    "\n",
    "# Model Performance on Holdout\n",
    "print(\n",
    "    classification_report(y_test, \n",
    "                          pipeline2.predict(docs_test), \n",
    "                          target_names=dataset.target_names)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model explanations\n",
    "Here, we'll wrap each pipeline into a Skater model object. We'll use this model object to generate LIME explanations in HTML to help better understand how each model makes predictions. We'll wrap this functionality into an ipywidget to allow the user the (a) modify the text and (b) toggle between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## You may need to enable ipywidgets\n",
    "! pip install ipywidgets\n",
    "!jupyter nbextension enable --py --user widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Create the explorer app.\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "from ipywidgets import Button, Textarea, Layout, Box, Label, Text, Output, RadioButtons, HBox\n",
    "from IPython.display import display, HTML, clear_output\n",
    "from skater.core.local_interpretation.lime.lime_text import LimeTextExplainer\n",
    "\n",
    "class TextExplainer(object):\n",
    "    def __init__(self, models, init_pattern=\"\"):\n",
    "        \"\"\"\n",
    "        Display box for LIME results.\n",
    "        \n",
    "        models: dictionary of skater of models.\n",
    "            Keys correspond to user-defined model names, used for radio buttons.\n",
    "            Values are skater models used to generate predictions.\n",
    "    \n",
    "        \"\"\"\n",
    "        self.status = \"Ready\"\n",
    "        self.explainer = LimeTextExplainer(class_names=dataset.target_names)\n",
    "        self.models = models\n",
    "        self.model_names = list(self.models.keys())\n",
    "        self.text_field = Textarea(init_pattern, layout=Layout(height='200px', width='500px'))\n",
    "        self.text_box = Box([Label(value='Text Box'), self.text_field])\n",
    "        \n",
    "        self.status_field = Label(self.status, layout=Layout(height='50px', width='100px'))        \n",
    "        self.status_box = Box([Label(value='Status'), self.status_field])\n",
    "\n",
    "        self.match_button = Button(description='Explain', )\n",
    "        self.match_button.on_click(self.match_pattern)\n",
    "        \n",
    "        self.model_selectors = RadioButtons(\n",
    "            options = self.model_names,\n",
    "            description = \"Use Model\"\n",
    "        )\n",
    "        \n",
    "        self.inputs_box = HBox([self.text_box, self.model_selectors])        \n",
    "        \n",
    "        self.explanation_area = Output()\n",
    "        display(self.inputs_box)       \n",
    "        display(self.match_button)\n",
    "        display(self.status_box)\n",
    "        display(self.explanation_area)\n",
    "        \n",
    "    @property\n",
    "    def model(self):\n",
    "        return self.models[self.model_selectors.value]\n",
    "            \n",
    "    @property\n",
    "    def text(self):\n",
    "        return self.text_field.value    \n",
    "    \n",
    "    def match_pattern(self, b):\n",
    "        self.status_field.value = 'loading'\n",
    "        with self.explanation_area:\n",
    "            clear_output()\n",
    "            display(HTML(self.get_explanation_as_html(self.text)))\n",
    "        self.status_field.value = 'ready'\n",
    "\n",
    "    def get_explanation_as_html(self, text):\n",
    "        \n",
    "        # generate most likely class to confine LIME results\n",
    "\n",
    "        explanation = self.explainer.explain_instance(text, \n",
    "                                                      self.model, \n",
    "                                                      top_labels=1)\n",
    "\n",
    "        return explanation.as_html()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea03cc8e0f7749bb8e547017e9feafd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6f97bfb13de45af8c310e77ceeeae5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "753901cd1cee4f088e4adf5eeacd1b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fe4427b3d10407fb76ff09341ce33a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = {\"CNN\": pipeline2.predict_proba, ' GBC-Pretrain': pipeline.predict_proba}\n",
    "r = TextExplainer(models, docs_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
