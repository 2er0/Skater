

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Interpretation Objects &mdash; skater 0 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Model Objects" href="model.html" />
    <link rel="prev" title="API Reference" href="../api.html" /> 

  
  <script src="../_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="../index.html" class="icon icon-home"> skater
          

          
          </a>

          
            
            
              <div class="version">
                1.1.0-b1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../install.html">Install Skater</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorial.html">Tutorial</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../api.html">API Reference</a><ul class="current">
<li class="toctree-l2 current"><a class="current reference internal" href="#">Interpretation Objects</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l3"><a class="reference internal" href="#loading-data">Loading Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="#global-interpretations">Global Interpretations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#feature-importance">Feature Importance</a></li>
<li class="toctree-l4"><a class="reference internal" href="#partial-dependence">Partial Dependence</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#local-interpretations">Local Interpretations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#lime">LIME</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="model.html">Model Objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="data.html">DataManagers</a></li>
</ul>
</li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">skater</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
          <li><a href="../api.html">API Reference</a> &raquo;</li>
        
      <li>Interpretation Objects</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/reference/interpretation.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="interpretation-objects">
<h1>Interpretation Objects<a class="headerlink" href="#interpretation-objects" title="Permalink to this headline">¶</a></h1>
<div class="section" id="overview">
<span id="interpretation-overview"></span><h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">¶</a></h2>
<p>Interpretation are initialized with a DataManager object, and expose interpretation algorithms as methods. For instance:</p>
<div class="highlight-python"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre>1
2
3
4</pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">skater</span> <span class="kn">import</span> <span class="n">Interpretation</span><span class="p">()</span>
<span class="n">interpreter</span> <span class="o">=</span> <span class="n">Interpretation</span><span class="p">()</span>
<span class="n">interpreter</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="n">interpreter</span><span class="o">.</span><span class="n">feature_importance</span><span class="o">.</span><span class="n">feature_importance</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</td></tr></table></div>
</div>
<div class="section" id="loading-data">
<h2>Loading Data<a class="headerlink" href="#loading-data" title="Permalink to this headline">¶</a></h2>
<p>Before running interpretation algorithms on a model, the Interpretation object usually needs data, either to learn about
the distribution of the training set or to pass inputs into a prediction function.</p>
<p>When calling Interpretation.load_data, the object creates a DataManager object, which handles the data, keeping track of feature
and observation names, as well as providing various sampling algorithms.</p>
<p>Currently load_data requires a numpy ndarray or pandas DataFrame, though we may add support for additional data structures in the future.
For more details on what the DataManager does, please see the relevant documentation [PROVIDE LINK].</p>
<dl class="method">
<dt id="skater.core.explanations.Interpretation.load_data">
<code class="descclassname">Interpretation.</code><code class="descname">load_data</code><span class="sig-paren">(</span><em>training_data</em>, <em>training_labels=None</em>, <em>feature_names=None</em>, <em>index=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skater/core/explanations.html#Interpretation.load_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skater.core.explanations.Interpretation.load_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Creates a DataSet object from inputs, ties to interpretation object.
This will be exposed to all submodules.</p>
<dl class="docutils">
<dt>training_data: numpy.ndarray, pandas.DataFrame</dt>
<dd>the dataset. can be 1D or 2D</dd>
<dt>feature_names: array-type</dt>
<dd>names to call features.</dd>
<dt>index: array-type</dt>
<dd>names to call rows.</dd>
</dl>
<blockquote>
<div>None</div></blockquote>
</dd></dl>

</div>
<div class="section" id="global-interpretations">
<h2>Global Interpretations<a class="headerlink" href="#global-interpretations" title="Permalink to this headline">¶</a></h2>
<p>A predictive model is a mapping from an input space to an output space. Interpretation algorithms
are divided into those that offer statistics and metrics on regions of the domain, such as the
marginal distribution of a feature, or the joint distribution of the entire training set.
In an ideal world there would exist some representation that would allow a human
to interpret a decision function in any number of dimensions. Given that we generally can only
intuit visualizations of a few dimensions at time, global interpretation algorithms either aggregate
or subset the feature space.</p>
<p>Currently, model agnostic global interpretation algorithms supported by skater include
partial dependence and feature importance.</p>
<div class="section" id="feature-importance">
<span id="interpretation-feature-importance"></span><h3>Feature Importance<a class="headerlink" href="#feature-importance" title="Permalink to this headline">¶</a></h3>
<p>Feature importance is generic term for the degree to which a predictive model relies on a particular
feature. skater feature importance implementation is based on an information theoretic criteria,
measuring the entropy in the change of predictions, given a perturbation of a given feature.
The intuition is that the more a model’s decision criteria depend on a feature, the
more we’ll see predictions change as a function of perturbing a feature.</p>
<dl class="class">
<dt id="skater.core.global_interpretation.feature_importance.FeatureImportance">
<em class="property">class </em><code class="descclassname">skater.core.global_interpretation.feature_importance.</code><code class="descname">FeatureImportance</code><span class="sig-paren">(</span><em>interpreter</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skater/core/global_interpretation/feature_importance.html#FeatureImportance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skater.core.global_interpretation.feature_importance.FeatureImportance" title="Permalink to this definition">¶</a></dt>
<dd><p>Contains methods for feature importance. Subclass of BaseGlobalInterpretation.</p>
<dl class="method">
<dt id="skater.core.global_interpretation.feature_importance.FeatureImportance.feature_importance">
<code class="descname">feature_importance</code><span class="sig-paren">(</span><em>model_instance</em>, <em>ascending=True</em>, <em>filter_classes=None</em>, <em>n_jobs=-1</em>, <em>progressbar=True</em>, <em>n_samples=5000</em>, <em>method='prediction-variance'</em>, <em>scorer_type='default'</em>, <em>use_scaling=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skater/core/global_interpretation/feature_importance.html#FeatureImportance.feature_importance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skater.core.global_interpretation.feature_importance.FeatureImportance.feature_importance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes feature importance of all features related to a model instance.
Supports classification, multi-class classification, and regression.</p>
<p>Wei, Pengfei, Zhenzhou Lu, and Jingwen Song.
“Variable Importance Analysis: A Comprehensive Review”.
Reliability Engineering &amp; System Safety 142 (2015): 399-432.</p>
<dl class="docutils">
<dt>model_instance: skater.model.model.Model subtype</dt>
<dd>the machine learning model “prediction” function to explain, such that
predictions = predict_fn(data).</dd>
<dt>ascending: boolean, default True</dt>
<dd>Helps with ordering Ascending vs Descending</dd>
<dt>filter_classes: array type</dt>
<dd>The classes to run partial dependence on. Default None invokes all classes.
Only used in classification models.</dd>
<dt>n_jobs: int</dt>
<dd>How many concurrent processes to use. Defaults -1, which grabs as many as are available.
Use 1 to avoid multiprocessing altogether.</dd>
<dt>progressbar: bool</dt>
<dd>Whether to display progress. This affects which function we use to operate on the pool
of processes, where including the progress bar results in 10-20% slowdowns.</dd>
<dt>n_samples: int</dt>
<dd>How many samples to use when computing importance.</dd>
<dt>method: string (default ‘prediction-variance’; ‘model-scoring’ for estimator specific scoring metric</dt>
<dd>How to compute feature importance. ‘model-scoring’ requires Interpretation.training_labels.
Note this choice should only rarely makes any significant differences
prediction-variance: mean absolute value of changes in predictions, given perturbations.
model-scoring: difference in log_loss or MAE of training_labels given perturbations.</dd>
<dt>scorer_type: string</dt>
<dd><p class="first">only used when method=’model-scoring’, and in this case defines which scoring function to use.
Default value is ‘default’, which evaluates to:</p>
<blockquote>
<div>regressors: mean absolute error
classifiers with probabilities: cross entropy
classifiers without probabilities: f1 score</div></blockquote>
<p class="last">See Skater.model.scorers for details.</p>
</dd>
<dt>use_scaling: bool</dt>
<dd>Whether to weight the importance values by the strength of the perturbations.
Generally doesn’t effect results unless n_samples is very small.</dd>
</dl>
<p>importances : Sorted Series</p>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skater.model</span> <span class="k">import</span> <span class="n">InMemoryModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skater.core.explanations</span> <span class="k">import</span> <span class="n">Interpretation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">InMemoryModel</span><span class="p">(</span><span class="n">rf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">,</span> <span class="n">examples</span> <span class="o">=</span> <span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span> <span class="o">=</span> <span class="n">Interpretation</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span><span class="o">.</span><span class="n">feature_importance</span><span class="o">.</span><span class="n">feature_importance</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="skater.core.global_interpretation.feature_importance.FeatureImportance.plot_feature_importance">
<code class="descname">plot_feature_importance</code><span class="sig-paren">(</span><em>modelinstance</em>, <em>filter_classes=None</em>, <em>ascending=True</em>, <em>ax=None</em>, <em>progressbar=True</em>, <em>n_jobs=-1</em>, <em>n_samples=5000</em>, <em>method='prediction-variance'</em>, <em>scorer_type='default'</em>, <em>use_scaling=False</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skater/core/global_interpretation/feature_importance.html#FeatureImportance.plot_feature_importance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skater.core.global_interpretation.feature_importance.FeatureImportance.plot_feature_importance" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes feature importance of all features related to a model instance,
then plots the results. Supports classification, multi-class classification, and regression.</p>
<dl class="docutils">
<dt>modelinstance: skater.model.model.Model subtype</dt>
<dd>estimator “prediction” function to explain the predictive model. Could be probability scores
or target values</dd>
<dt>filter_classes: array type</dt>
<dd>The classes to run partial dependence on. Default None invokes all classes.
Only used in classification models.</dd>
<dt>ascending: boolean, default True</dt>
<dd>Helps with ordering Ascending vs Descending</dd>
<dt>ax: matplotlib.axes._subplots.AxesSubplot</dt>
<dd>existing subplot on which to plot feature importance. If none is provided,
one will be created.</dd>
<dt>progressbar: bool</dt>
<dd>Whether to display progress. This affects which function we use to operate on the pool
of processes, where including the progress bar results in 10-20% slowdowns.</dd>
<dt>n_jobs: int</dt>
<dd>How many concurrent processes to use. Defaults -1, which grabs as many as are available.
Use 1 to avoid multiprocessing altogether.</dd>
<dt>n_samples: int</dt>
<dd>How many samples to use when computing importance.</dd>
<dt>method: string</dt>
<dd>How to compute feature importance. ‘model-scoring’ requires Interpretation.training_labels
prediction-variance: mean absolute value of changes in predictions, given perturbations.
model-scoring: difference in log_loss or MAE of training_labels given perturbations.
Note this vary rarely makes any significant differences</dd>
<dt>scorer_type: string</dt>
<dd><p class="first">only used when method=’model-scoring’, and in this case defines which scoring function to use.
Default value is ‘default’, which evaluates to:</p>
<blockquote>
<div>regressors: mean absolute error
classifiers with probabilities: cross entropy
classifiers without probabilities: f1 score</div></blockquote>
<p class="last">See Skater.model.scorers for details.</p>
</dd>
<dt>use_scaling: bool</dt>
<dd>Whether to weight the importance values by the strength of the perturbations.
Generally doesn’t effect results unless n_samples is very small.</dd>
</dl>
<p>f: figure instance
ax: matplotlib.axes._subplots.AxesSubplot</p>
<blockquote>
<div>could be used to for further modification to the plots</div></blockquote>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skater.model</span> <span class="k">import</span> <span class="n">InMemoryModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skater.core.explanations</span> <span class="k">import</span> <span class="n">Interpretation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">InMemoryModel</span><span class="p">(</span><span class="n">rf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">,</span> <span class="n">examples</span> <span class="o">=</span> <span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span> <span class="o">=</span> <span class="n">Interpretation</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span><span class="o">.</span><span class="n">feature_importance</span><span class="o">.</span><span class="n">plot_feature_importance</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="partial-dependence">
<span id="interpretation-partial-dependence"></span><h3>Partial Dependence<a class="headerlink" href="#partial-dependence" title="Permalink to this headline">¶</a></h3>
<p>Partial Dependence describes the marginal impact of a feature on model prediction, holding
other features in the model constant. The derivative of partial dependence describes the impact of a feature (analogous to a feature coefficient
in a regression model).</p>
<dl class="class">
<dt id="skater.core.global_interpretation.partial_dependence.PartialDependence">
<em class="property">class </em><code class="descclassname">skater.core.global_interpretation.partial_dependence.</code><code class="descname">PartialDependence</code><span class="sig-paren">(</span><em>interpreter</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skater/core/global_interpretation/partial_dependence.html#PartialDependence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skater.core.global_interpretation.partial_dependence.PartialDependence" title="Permalink to this definition">¶</a></dt>
<dd><p>Contains methods for partial dependence. Subclass of BaseGlobalInterpretation</p>
<p>Partial dependence adapted from:</p>
<p>T. Hastie, R. Tibshirani and J. Friedman,
Elements of Statistical Learning Ed. 2, Springer, 2009.</p>
<dl class="staticmethod">
<dt id="skater.core.global_interpretation.partial_dependence.PartialDependence.compute_3d_gradients">
<em class="property">static </em><code class="descname">compute_3d_gradients</code><span class="sig-paren">(</span><em>pdp</em>, <em>mean_col</em>, <em>feature_1</em>, <em>feature_2</em>, <em>scaled=True</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skater/core/global_interpretation/partial_dependence.html#PartialDependence.compute_3d_gradients"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skater.core.global_interpretation.partial_dependence.PartialDependence.compute_3d_gradients" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes component-wise gradients of pdp dataframe.</p>
<dl class="docutils">
<dt>pdp: pandas.DataFrame</dt>
<dd>DataFrame containing partial dependence values</dd>
<dt>mean_col: string</dt>
<dd>column name corresponding to pdp value</dd>
<dt>feature_1: string</dt>
<dd>column name corresponding to feature 1</dd>
<dt>feature_2: string</dt>
<dd>column name corresponding to feature 2</dd>
<dt>scaled: bool</dt>
<dd>Whether to scale the x1 and x2 gradients relative to x1 and x2 bin sizes</dd>
</dl>
<p>dx, dy, x_matrix, y_matrix, z_matrix</p>
</dd></dl>

<dl class="method">
<dt id="skater.core.global_interpretation.partial_dependence.PartialDependence.partial_dependence">
<code class="descname">partial_dependence</code><span class="sig-paren">(</span><em>feature_ids</em>, <em>modelinstance</em>, <em>filter_classes=None</em>, <em>grid=None</em>, <em>grid_resolution=30</em>, <em>n_jobs=-1</em>, <em>grid_range=None</em>, <em>sample=True</em>, <em>sampling_strategy='random-choice'</em>, <em>n_samples=1000</em>, <em>bin_count=50</em>, <em>return_metadata=False</em>, <em>progressbar=True</em>, <em>variance_type='estimate'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skater/core/global_interpretation/partial_dependence.html#PartialDependence.partial_dependence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skater.core.global_interpretation.partial_dependence.PartialDependence.partial_dependence" title="Permalink to this definition">¶</a></dt>
<dd><p>Approximates the partial dependence of the predict_fn with respect to the
variables passed.</p>
<dl class="docutils">
<dt>feature_ids: list</dt>
<dd>the names/ids of the features for which partial dependence is to be computed.
Note that the algorithm’s complexity scales exponentially with additional
features, so generally one should only look at one or two features at a
time. These feature ids must be available in the class’s associated DataSet.
As of now, we only support looking at 1 or 2 features at a time.</dd>
<dt>modelinstance: skater.model.model.Model subtype</dt>
<dd><p class="first">an estimator function of a fitted model used to derive prediction. Supports
classification and regression. Supports classification(binary, multi-class) and regression.
predictions = predict_fn(data)</p>
<p class="last">Can either by a skater.model.remote.DeployedModel or a
skater.model.local.InMemoryModel</p>
</dd>
<dt>filter_classes: array type</dt>
<dd>The classes to run partial dependence on. Default None invokes all classes.
Only used in classification models.</dd>
<dt>grid: numpy.ndarray</dt>
<dd>2 dimensional array on which we fix values of features. Note this is
determined automatically if not given based on the percentiles of the
dataset.</dd>
<dt>grid_resolution: int</dt>
<dd>how many unique values to include in the grid. If the percentile range
is 5% to 95%, then that range will be cut into &lt;grid_resolution&gt;
equally size bins. Defaults to 30.</dd>
<dt>n_jobs: int</dt>
<dd>The number of CPUs to use to compute the PDs. -1 means ‘all CPUs’.
Defaults to using all cores(-1).</dd>
<dt>grid_range: tuple</dt>
<dd>the percentile extrama to consider. 2 element tuple, increasing, bounded
between 0 and 1.</dd>
<dt>sample: boolean</dt>
<dd>Whether to sample from the original dataset.</dd>
<dt>sampling_strategy: string</dt>
<dd>If sampling, which approach to take. See DataSet.generate_sample for
details.</dd>
<dt>n_samples: int</dt>
<dd>The number of samples to use from the original dataset. Note this is
only active if sample = True and sampling strategy = ‘uniform’. If
using ‘uniform-over-similarity-ranks’, use samples per bin</dd>
<dt>bin_count: int</dt>
<dd>The number of bins to use when using the similarity based sampler. Note
this is only active if sample = True and
sampling_strategy = ‘uniform-over-similarity-ranks’.
total samples = bin_count * samples per bin.</dd>
<dt>samples_per_bin: int</dt>
<dd>The number of samples to collect for each bin within the sampler. Note
this is only active if sample = True and
sampling_strategy = ‘uniform-over-similarity-ranks’. If using
sampling_strategy = ‘uniform’, use n_samples.
total samples = bin_count * samples per bin.</dd>
</dl>
<p>variance_type: string</p>
<p>return_metadata: boolean</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Example:</th><td class="field-body"></td>
</tr>
</tbody>
</table>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skater.model</span> <span class="k">import</span> <span class="n">InMemoryModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skater.core.explanations</span> <span class="k">import</span> <span class="n">Interpretation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">RandomForestClassier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="k">import</span> <span class="n">load_boston</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">boston</span> <span class="o">=</span> <span class="n">load_boston</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">target</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">features</span> <span class="o">=</span> <span class="n">boston</span><span class="o">.</span><span class="n">feature_names</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rf</span> <span class="o">=</span> <span class="n">RandomForestClassier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">InMemoryModel</span><span class="p">(</span><span class="n">rf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">,</span> <span class="n">examples</span> <span class="o">=</span> <span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span> <span class="o">=</span> <span class="n">Interpretation</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">feature_ids</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ZN&#39;</span><span class="p">,</span><span class="s1">&#39;CRIM&#39;</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span><span class="o">.</span><span class="n">partial_dependence</span><span class="o">.</span><span class="n">partial_dependence</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

<dl class="method">
<dt id="skater.core.global_interpretation.partial_dependence.PartialDependence.plot_partial_dependence">
<code class="descname">plot_partial_dependence</code><span class="sig-paren">(</span><em>feature_ids</em>, <em>modelinstance</em>, <em>filter_classes=None</em>, <em>grid=None</em>, <em>grid_resolution=30</em>, <em>grid_range=None</em>, <em>n_jobs=-1</em>, <em>sample=True</em>, <em>sampling_strategy='random-choice'</em>, <em>n_samples=1000</em>, <em>bin_count=50</em>, <em>with_variance=False</em>, <em>figsize=(16</em>, <em>10)</em>, <em>progressbar=True</em>, <em>variance_type='estimate'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/skater/core/global_interpretation/partial_dependence.html#PartialDependence.plot_partial_dependence"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skater.core.global_interpretation.partial_dependence.PartialDependence.plot_partial_dependence" title="Permalink to this definition">¶</a></dt>
<dd><p>Computes partial_dependence of a set of variables. Essentially approximates
the partial partial_dependence of the predict_fn with respect to the variables
passed.</p>
<dl class="docutils">
<dt>feature_ids: list</dt>
<dd>the names/ids of the features for which partial dependence is to be computed.
Note that the algorithm’s complexity scales exponentially with additional
features, so generally one should only look at one or two features at a
time. These feature ids must be available in the class’s associated DataSet.
As of now, we only support looking at 1 or 2 features at a time.</dd>
<dt>modelinstance: skater.model.model.Model subtype</dt>
<dd><p class="first">an estimator function of a fitted model used to derive prediction. Supports
classification and regression. Supports classification(binary, multi-class) and regression.
predictions = predict_fn(data)</p>
<p class="last">Can either by a skater.model.remote.DeployedModel or a
skater.model.local.InMemoryModel</p>
</dd>
<dt>grid: numpy.ndarray</dt>
<dd>2 dimensional array on which we fix values of features. Note this is
determined automatically if not given based on the percentiles of the
dataset.</dd>
<dt>grid_resolution: int</dt>
<dd>how many unique values to include in the grid. If the percentile range
is 5% to 95%, then that range will be cut into &lt;grid_resolution&gt;
equally size bins. Defaults to 30.</dd>
<dt>grid_range: tuple</dt>
<dd>the percentile extrama to consider. 2 element tuple, increasing, bounded
between 0 and 1.</dd>
<dt>n_jobs: int</dt>
<dd>The number of CPUs to use to compute the PDs. -1 means ‘all CPUs’.
Defaults to using all cores(-1).</dd>
<dt>sample: boolean</dt>
<dd>Whether to sample from the original dataset.</dd>
<dt>sampling_strategy: string</dt>
<dd>If sampling, which approach to take. See DataSet.generate_sample for
details.</dd>
<dt>n_samples: int</dt>
<dd>The number of samples to use from the original dataset. Note this is
only active if sample = True and sampling strategy = ‘uniform’. If
using ‘uniform-over-similarity-ranks’, use samples per bin</dd>
<dt>bin_count: int</dt>
<dd>The number of bins to use when using the similarity based sampler. Note
this is only active if sample = True and
sampling_strategy = ‘uniform-over-similarity-ranks’.
total samples = bin_count * samples per bin.</dd>
<dt>samples_per_bin: int</dt>
<dd>The number of samples to collect for each bin within the sampler. Note
this is only active if sample = True and
sampling_strategy = ‘uniform-over-similarity-ranks’. If using
sampling_strategy = ‘uniform’, use n_samples.
total samples = bin_count * samples per bin.</dd>
<dt>with_variance(bool):</dt>
<dd>whether to include pdp error bars in the plots. Currently disabled for 3D
plots for visibility. If you have a use case where you’d like error bars for
3D pdp plots, let us know!</dd>
<dt>plot_title(string):</dt>
<dd>title for pdp plots</dd>
<dt>variance_type: string</dt>
<dd>if variance plotting is enabled, determines which variance to include.
estimate: the variance of the partial dependence estimates
prediction: the variances of the predictions at the given point</dd>
</dl>
<div class="highlight-default"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="k">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets.california_housing</span> <span class="k">import</span> <span class="n">fetch_california_housing</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cal_housing</span> <span class="o">=</span> <span class="n">fetch_california_housing</span><span class="p">()</span>
<span class="go"># split 80/20 train-test</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x_train</span><span class="p">,</span> <span class="n">x_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">cal_housing</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">cal_housing</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">names</span> <span class="o">=</span> <span class="n">cal_housing</span><span class="o">.</span><span class="n">feature_names</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training the estimator...&quot;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                            <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;huber&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skater.core.explanations</span> <span class="k">import</span> <span class="n">Interpretation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span> <span class="o">=</span> <span class="n">Interpretation</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Feature name: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">names</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">names</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input feature name: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">[</span><span class="n">names</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">names</span><span class="p">[</span><span class="mi">5</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">skater.model</span> <span class="k">import</span> <span class="n">InMemoryModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">InMemoryModel</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">,</span> <span class="n">examples</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">interpreter</span><span class="o">.</span><span class="n">partial_dependence</span><span class="o">.</span><span class="n">plot_partial_dependence</span><span class="p">([</span><span class="n">names</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">names</span><span class="p">[</span><span class="mi">5</span><span class="p">]],</span> <span class="n">model</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>                                                        <span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="local-interpretations">
<span id="interpretation-overview-local"></span><h2>Local Interpretations<a class="headerlink" href="#local-interpretations" title="Permalink to this headline">¶</a></h2>
<p>Local interpretations are based on using interpretable surrogate models to illustrate
how features impact predictions constrained to a particular point or small region in
the input space. Linear surrogates around a point correspond the LIME algorithm; tree like
surrogates around a point correspond to anchorLIME.</p>
<div class="section" id="lime">
<h3>LIME<a class="headerlink" href="#lime" title="Permalink to this headline">¶</a></h3>
<dl class="class">
<dt id="skater.core.local_interpretation.lime.lime_tabular.LimeTabularExplainer">
<em class="property">class </em><code class="descclassname">skater.core.local_interpretation.lime.lime_tabular.</code><code class="descname">LimeTabularExplainer</code><span class="sig-paren">(</span><em>training_data</em>, <em>mode='classification'</em>, <em>training_labels=None</em>, <em>feature_names=None</em>, <em>categorical_features=None</em>, <em>categorical_names=None</em>, <em>kernel_width=None</em>, <em>verbose=False</em>, <em>class_names=None</em>, <em>feature_selection='auto'</em>, <em>discretize_continuous=True</em>, <em>discretizer='quartile'</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lime/lime_tabular.html#LimeTabularExplainer"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skater.core.local_interpretation.lime.lime_tabular.LimeTabularExplainer" title="Permalink to this definition">¶</a></dt>
<dd><p>Explains predictions on tabular (i.e. matrix) data.
For numerical features, perturb them by sampling from a Normal(0,1) and
doing the inverse operation of mean-centering and scaling, according to the
means and stds in the training data. For categorical features, perturb by
sampling according to the training distribution, and making a binary
feature that is 1 when the value is the same as the instance being
explained.</p>
<dl class="method">
<dt id="skater.core.local_interpretation.lime.lime_tabular.LimeTabularExplainer.explain_instance">
<code class="descname">explain_instance</code><span class="sig-paren">(</span><em>data_row</em>, <em>predict_fn</em>, <em>labels=(1</em>, <em>)</em>, <em>top_labels=None</em>, <em>num_features=10</em>, <em>num_samples=5000</em>, <em>distance_metric='euclidean'</em>, <em>model_regressor=None</em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/lime/lime_tabular.html#LimeTabularExplainer.explain_instance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#skater.core.local_interpretation.lime.lime_tabular.LimeTabularExplainer.explain_instance" title="Permalink to this definition">¶</a></dt>
<dd><p>Generates explanations for a prediction.</p>
<p>First, we generate neighborhood data by randomly perturbing features
from the instance (see __data_inverse). We then learn locally weighted
linear models on this neighborhood data to explain each of the classes
in an interpretable way (see lime_base.py).</p>
<dl class="docutils">
<dt>Args:</dt>
<dd><p class="first">data_row: 1d numpy array, corresponding to a row
predict_fn: prediction function. For classifiers, this should be a</p>
<blockquote>
<div><p>function that takes a numpy array and outputs prediction
probabilities. For regressors, this takes a numpy array and
returns the predictions. For ScikitClassifiers, this is</p>
<blockquote>
<div><cite>classifier.predict_proba()</cite>. For ScikitRegressors, this
is <cite>regressor.predict()</cite>.</div></blockquote>
</div></blockquote>
<p>labels: iterable with labels to be explained.
top_labels: if not None, ignore labels and produce explanations for</p>
<blockquote>
<div>the K labels with highest prediction probabilities, where K is
this parameter.</div></blockquote>
<p class="last">num_features: maximum number of features present in explanation
num_samples: size of the neighborhood to learn the linear model
distance_metric: the distance metric to use for weights.
model_regressor: sklearn regressor to use in explanation. Defaults
to Ridge regression in LimeBase. Must have <a href="#id1"><span class="problematic" id="id2">model_regressor.coef_</span></a>
and ‘sample_weight’ as a parameter to model_regressor.fit()</p>
</dd>
<dt>Returns:</dt>
<dd>An Explanation object (see explanation.py) with the corresponding
explanations.</dd>
</dl>
</dd></dl>

</dd></dl>

</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="model.html" class="btn btn-neutral float-right" title="Model Objects" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../api.html" class="btn btn-neutral" title="API Reference" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Author.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../',
            VERSION:'0',
            LANGUAGE:'en',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="../_static/jquery.js"></script>
      <script type="text/javascript" src="../_static/underscore.js"></script>
      <script type="text/javascript" src="../_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="../_static/js/theme.js"></script>
  

  <script type="text/javascript">
      jQuery(function () {
          
          SphinxRtdTheme.Navigation.enableSticky();
          
      });
  </script> 

</body>
</html>